{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb7a3cf",
   "metadata": {},
   "source": [
    "# Tokenization (A_bytes + B_raw/B_norm) — Notebook\n",
    "\n",
    "- **A_bytes**: контрольный вариант (byte-level UTF-8)\n",
    "- **B_raw / B_norm**: основной вариант (структурные токены Python + словарь по частоте)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2004907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import ast\n",
    "import hashlib\n",
    "import io\n",
    "import json\n",
    "import keyword\n",
    "import re\n",
    "import tokenize\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, Iterator, List, Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0d9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Helpers: IO\n",
    "# -------------------------\n",
    "\n",
    "def read_jsonl(path: Path) -> Iterator[dict]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "\n",
    "def write_jsonl(path: Path, rows: Iterable[dict]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def write_json(path: Path, obj: dict) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc3754cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Schema: row adapters\n",
    "# -------------------------\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RowView:\n",
    "    repo: str\n",
    "    path: str\n",
    "    func_name: str\n",
    "    doc: str\n",
    "    code: str\n",
    "    sha: str\n",
    "    language: str\n",
    "    keep: Optional[bool]\n",
    "    parsable: Optional[bool]\n",
    "\n",
    "\n",
    "def _get_first(d: dict, keys: List[str], default=None):\n",
    "    for k in keys:\n",
    "        if k in d and d[k] is not None:\n",
    "            return d[k]\n",
    "    return default\n",
    "\n",
    "\n",
    "def adapt_row(d: dict) -> Optional[RowView]:\n",
    "    \"\"\"\n",
    "    Supports at least:\n",
    "    - CodeXGLUE code-to-text: repo/path/func_name/docstring/code_clean/sha256/language/keep/parsable\n",
    "    - CodeSearchNet: repository_name/func_path_in_repository/func_name/docstring/code_clean/sha256/language/keep/parsable\n",
    "    \"\"\"\n",
    "    language = (d.get(\"language\") or \"\").lower()\n",
    "    keep = d.get(\"keep\")\n",
    "    parsable = d.get(\"parsable\")\n",
    "\n",
    "    doc = _get_first(d, [\"docstring\", \"func_documentation_string\"], default=\"\")\n",
    "    code = _get_first(d, [\"code_clean\", \"func_code_string\", \"code\"], default=\"\")\n",
    "\n",
    "    repo = _get_first(d, [\"repository_name\", \"repo\", \"repository\"], default=\"\")\n",
    "    path = _get_first(d, [\"func_path_in_repository\", \"path\", \"func_code_url\"], default=\"\")\n",
    "    func_name = _get_first(d, [\"func_name\"], default=\"\")\n",
    "\n",
    "    sha = _get_first(d, [\"sha256\", \"sha\"], default=\"\")\n",
    "    if not sha:\n",
    "        sha = hashlib.sha256((repo + \"\\n\" + path + \"\\n\" + code).encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "\n",
    "    if not isinstance(doc, str) or not isinstance(code, str):\n",
    "        return None\n",
    "    if not doc.strip() or not code.strip():\n",
    "        return None\n",
    "\n",
    "    return RowView(\n",
    "        repo=repo, path=path, func_name=func_name,\n",
    "        doc=doc, code=code, sha=sha,\n",
    "        language=language, keep=keep, parsable=parsable\n",
    "    )\n",
    "\n",
    "\n",
    "def default_filter(rv: RowView, language: str = \"python\") -> bool:\n",
    "    if language and rv.language and rv.language != language.lower():\n",
    "        return False\n",
    "    if rv.keep is False:\n",
    "        return False\n",
    "    # по умолчанию parsable не обязателен (зависит от датасета)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62d5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Text normalization (docstrings)\n",
    "# -------------------------\n",
    "\n",
    "_WS_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize_docstring(doc: str) -> str:\n",
    "    doc = (doc or \"\").strip()\n",
    "    doc = _WS_RE.sub(\" \", doc)\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab42d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Variant B: structured Python tokens\n",
    "# -------------------------\n",
    "\n",
    "def python_struct_tokens(code: str, mode: str = \"B_raw\", max_ids: int = 64) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize Python code into a reversible-ish sequence.\n",
    "\n",
    "    Emits:\n",
    "      - <nl>, <indent>, <dedent>\n",
    "      - raw token strings\n",
    "      - in B_norm mode: identifiers -> <id_k>, numbers -> <num>, strings -> <str>\n",
    "\n",
    "    Identifier scoping (B_norm):\n",
    "      - mapping is reset per function/code snippet\n",
    "      - first time we see a NAME (not keyword) that is not an attribute access, we assign <id_1>, <id_2>, ...\n",
    "      - same original name -> same <id_k> within the snippet\n",
    "      - attribute names after '.' are kept as-is (helps preserve API surface like obj.method)\n",
    "    \"\"\"\n",
    "    code = (code or \"\").expandtabs(4)\n",
    "\n",
    "    id_map: Dict[str, int] = {}\n",
    "    next_id = 1\n",
    "\n",
    "    out: List[str] = []\n",
    "    prev_token: Optional[str] = None\n",
    "\n",
    "    g = tokenize.generate_tokens(io.StringIO(code).readline)\n",
    "    for tok in g:\n",
    "        ttype, tstr = tok.type, tok.string\n",
    "\n",
    "        if ttype in (tokenize.NL, tokenize.NEWLINE):\n",
    "            out.append(\"<nl>\")\n",
    "            prev_token = \"<nl>\"\n",
    "            continue\n",
    "        if ttype == tokenize.INDENT:\n",
    "            out.append(\"<indent>\")\n",
    "            prev_token = \"<indent>\"\n",
    "            continue\n",
    "        if ttype == tokenize.DEDENT:\n",
    "            out.append(\"<dedent>\")\n",
    "            prev_token = \"<dedent>\"\n",
    "            continue\n",
    "        if ttype == tokenize.ENDMARKER:\n",
    "            break\n",
    "        if ttype == tokenize.COMMENT:\n",
    "            continue\n",
    "\n",
    "        if mode == \"B_norm\":\n",
    "            if ttype == tokenize.NAME and not keyword.iskeyword(tstr):\n",
    "                # Keep attributes after dot to preserve API surface:\n",
    "                if prev_token == \".\":\n",
    "                    out.append(tstr)\n",
    "                else:\n",
    "                    if tstr not in id_map and next_id <= max_ids:\n",
    "                        id_map[tstr] = next_id\n",
    "                        next_id += 1\n",
    "                    if tstr in id_map:\n",
    "                        out.append(f\"<id_{id_map[tstr]}>\")\n",
    "                    else:\n",
    "                        out.append(\"<id>\")\n",
    "                prev_token = tstr\n",
    "                continue\n",
    "            if ttype == tokenize.NUMBER:\n",
    "                out.append(\"<num>\")\n",
    "                prev_token = \"<num>\"\n",
    "                continue\n",
    "            if ttype == tokenize.STRING:\n",
    "                out.append(\"<str>\")\n",
    "                prev_token = \"<str>\"\n",
    "                continue\n",
    "\n",
    "        out.append(tstr)\n",
    "        prev_token = tstr\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---- detokenize (для проверки parsable после токенизации) ----\n",
    "\n",
    "_OPERATORS = {\n",
    "    \"+\",\"-\",\"*\",\"/\",\"//\",\"%\",\"**\",\n",
    "    \"==\",\"!=\",\">=\",\"<=\",\"<\",\">\",\n",
    "    \"=\", \"+=\",\"-=\",\"*=\",\"/=\",\"%=\",\"//=\",\"**=\",\n",
    "    \"(\",\")\",\"[\",\"]\",\"{\",\"}\",\n",
    "    \",\",\":\",\".\",\";\",\"@\",\n",
    "    \"and\",\"or\",\"not\",\"in\",\"is\",\"|\",\"&\",\"^\",\"~\",\n",
    "    \"<<\",\">>\",\n",
    "}\n",
    "\n",
    "def detokenize_python(tokens: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Rough reversible detokenizer for our structured token stream.\n",
    "\n",
    "    Goal: produce code string that is often AST-parsable (not perfect pretty-printer).\n",
    "    \"\"\"\n",
    "    lines: List[str] = []\n",
    "    cur: List[str] = []\n",
    "    indent = 0\n",
    "\n",
    "    def flush_line():\n",
    "        nonlocal cur\n",
    "        line = \" \" * (indent * 4) + \"\".join(cur).rstrip()\n",
    "        lines.append(line)\n",
    "        cur = []\n",
    "\n",
    "    prev: Optional[str] = None\n",
    "\n",
    "    for t in tokens:\n",
    "        if t == \"<nl>\":\n",
    "            flush_line()\n",
    "            prev = \"<nl>\"\n",
    "            continue\n",
    "        if t == \"<indent>\":\n",
    "            indent += 1\n",
    "            prev = \"<indent>\"\n",
    "            continue\n",
    "        if t == \"<dedent>\":\n",
    "            indent = max(0, indent - 1)\n",
    "            prev = \"<dedent>\"\n",
    "            continue\n",
    "\n",
    "        # B_norm placeholders -> make them syntactically valid\n",
    "        if t.startswith(\"<id_\") or t == \"<id>\":\n",
    "            t = \"v\"\n",
    "        elif t == \"<num>\":\n",
    "            t = \"0\"\n",
    "        elif t == \"<str>\":\n",
    "            t = \"''\"\n",
    "\n",
    "        # spacing heuristic\n",
    "        if not cur:\n",
    "            cur.append(t)\n",
    "            prev = t\n",
    "            continue\n",
    "\n",
    "        need_space = False\n",
    "        if prev in (\"<nl>\", \"<indent>\", \"<dedent>\"):\n",
    "            need_space = False\n",
    "        elif t in (\")\",\"]\",\"}\"):\n",
    "            need_space = False\n",
    "        elif prev in (\"(\", \"[\", \"{\", \".\", \",\", \":\"):\n",
    "            need_space = False\n",
    "        elif t == \".\":\n",
    "            need_space = False\n",
    "        else:\n",
    "            # default add space between NAME/NUMBER/keywords and others\n",
    "            if prev.isalnum() and t.isalnum():\n",
    "                need_space = True\n",
    "            elif prev.isalnum() and t == \"(\":\n",
    "                need_space = False\n",
    "            elif prev in _OPERATORS or t in _OPERATORS:\n",
    "                need_space = True\n",
    "            else:\n",
    "                need_space = True\n",
    "\n",
    "        if need_space:\n",
    "            cur.append(\" \")\n",
    "        cur.append(t)\n",
    "        prev = t\n",
    "\n",
    "    if cur or not lines:\n",
    "        flush_line()\n",
    "\n",
    "    # Remove trailing empty lines\n",
    "    while lines and not lines[-1].strip():\n",
    "        lines.pop()\n",
    "\n",
    "    return \"\\n\".join(lines) + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ecd723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Splitting (stable by repo)\n",
    "# -------------------------\n",
    "\n",
    "def split_by_repo(repo: str, train: float = 0.90, val: float = 0.05) -> str:\n",
    "    \"\"\"\n",
    "    Deterministic split by repository string.\n",
    "    \"\"\"\n",
    "    h = int(hashlib.sha256((repo or \"\").encode(\"utf-8\", errors=\"ignore\")).hexdigest(), 16)\n",
    "    r = (h % 10_000) / 10_000.0\n",
    "    if r < train:\n",
    "        return \"train\"\n",
    "    if r < train + val:\n",
    "        return \"val\"\n",
    "    return \"test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdd5196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Example builder: doc2code\n",
    "# -------------------------\n",
    "\n",
    "def build_example_tokens(rv: RowView, mode: str, max_id_vars: int = 64) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Returns (input_tokens, target_tokens) for doc2code task:\n",
    "        input  = <bos> <task> <doc> DOC </doc> <code>\n",
    "        target = CODE </code> <eos>\n",
    "    \"\"\"\n",
    "    doc = normalize_docstring(rv.doc)\n",
    "    doc_toks = doc.split()\n",
    "\n",
    "    code_toks = python_struct_tokens(rv.code, mode=mode, max_ids=max_id_vars)\n",
    "\n",
    "    input_toks = [\"<bos>\", \"<task:doc2code>\", \"<doc>\"] + doc_toks + [\"</doc>\", \"<code>\"]\n",
    "    target_toks = code_toks + [\"</code>\", \"<eos>\"]\n",
    "    return input_toks, target_toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7262308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Vocab & encoding (Variant B)\n",
    "# -------------------------\n",
    "\n",
    "SPECIAL_B = [\n",
    "    \"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\", \"<sep>\",\n",
    "    \"<nl>\", \"<indent>\", \"<dedent>\",\n",
    "    \"<mask>\", \"<num>\", \"<str>\", \"<id>\"\n",
    "]\n",
    "# We also allow <id_k> tokens dynamically (B_norm) - they go through <unk> unless present in vocab.\n",
    "\n",
    "def build_vocab(train_rows: List[RowView], mode: str, vocab_size: int, min_freq: int, max_id_vars: int = 64) -> Tuple[Dict[str,int], Dict[int,str], Counter]:\n",
    "    cnt = Counter()\n",
    "    for rv in train_rows:\n",
    "        inp, tgt = build_example_tokens(rv, mode, max_id_vars=max_id_vars)\n",
    "        cnt.update(inp)\n",
    "        cnt.update(tgt)\n",
    "\n",
    "    # start with specials + structural/task tags\n",
    "    base_special = SPECIAL_B + [\"<task:doc2code>\", \"<doc>\", \"</doc>\", \"<code>\", \"</code>\"]\n",
    "    token2id: Dict[str,int] = {}\n",
    "    for t in base_special:\n",
    "        if t not in token2id:\n",
    "            token2id[t] = len(token2id)\n",
    "\n",
    "    # remove base specials to avoid duplicates\n",
    "    for t in base_special:\n",
    "        cnt.pop(t, None)\n",
    "\n",
    "    # keep by frequency then lexicographic for stability\n",
    "    items = [(t,f) for t,f in cnt.items() if f >= min_freq]\n",
    "    items.sort(key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    for t,f in items:\n",
    "        if len(token2id) >= vocab_size:\n",
    "            break\n",
    "        token2id[t] = len(token2id)\n",
    "\n",
    "    id2token = {i:t for t,i in token2id.items()}\n",
    "    return token2id, id2token, cnt\n",
    "\n",
    "\n",
    "def encode_tokens(tokens: List[str], token2id: Dict[str,int]) -> Tuple[List[int], int]:\n",
    "    unk_id = token2id.get(\"<unk>\")\n",
    "    oov = 0\n",
    "    ids = []\n",
    "    for t in tokens:\n",
    "        i = token2id.get(t)\n",
    "        if i is None:\n",
    "            ids.append(unk_id)\n",
    "            oov += 1\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids, oov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "247cbeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Variant A: byte-level baseline\n",
    "# -------------------------\n",
    "\n",
    "SPECIAL_A = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\", \"<sep>\"]  # keep minimal, fixed\n",
    "BYTE_OFFSET = len(SPECIAL_A)  # bytes 0..255 map to ids [BYTE_OFFSET..BYTE_OFFSET+255]\n",
    "\n",
    "def build_line_text(rv: RowView) -> str:\n",
    "    # One-line \"doc + code\" for baseline. Keep it simple and deterministic.\n",
    "    doc = normalize_docstring(rv.doc)\n",
    "    code = (rv.code or \"\").strip()\n",
    "    return f\"<doc> {doc} </doc> <code> {code} </code>\"\n",
    "\n",
    "def encode_bytes(text: str, max_len: int) -> Tuple[List[int], int, bool]:\n",
    "    \"\"\"\n",
    "    Returns (ids, oov_count, truncated) for baseline A.\n",
    "    \"\"\"\n",
    "    b = (text or \"\").encode(\"utf-8\", errors=\"replace\")\n",
    "    # +2 for <bos>/<eos>\n",
    "    truncated = False\n",
    "    if len(b) + 2 > max_len:\n",
    "        b = b[: max_len - 2]\n",
    "        truncated = True\n",
    "    ids = [SPECIAL_A.index(\"<bos>\")] + [BYTE_OFFSET + x for x in b] + [SPECIAL_A.index(\"<eos>\")]\n",
    "    return ids, 0, truncated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce48b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Metrics helpers\n",
    "# -------------------------\n",
    "\n",
    "def safe_ast_parse(code: str) -> bool:\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def quantiles_int(xs: List[int]) -> Dict[str,int]:\n",
    "    if not xs:\n",
    "        return {\"p50\": 0, \"p90\": 0, \"p95\": 0, \"p99\": 0, \"min\": 0, \"max\": 0}\n",
    "    xs = sorted(xs)\n",
    "    def q(p: float) -> int:\n",
    "        i = int(round((len(xs)-1) * p))\n",
    "        return int(xs[max(0, min(len(xs)-1, i))])\n",
    "    return {\n",
    "        \"min\": int(xs[0]),\n",
    "        \"p50\": q(0.50),\n",
    "        \"p90\": q(0.90),\n",
    "        \"p95\": q(0.95),\n",
    "        \"p99\": q(0.99),\n",
    "        \"max\": int(xs[-1]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a707b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Pipeline runner (notebook-friendly)\n",
    "# -------------------------\n",
    "\n",
    "def run_tokenization(\n",
    "    input_path: str,\n",
    "    out_dir: str = \"out_tokenization\",\n",
    "    language: str = \"python\",\n",
    "    max_len: int = 2048,\n",
    "    train_ratio: float = 0.90,\n",
    "    val_ratio: float = 0.05,\n",
    "    vocab_size: int = 32000,\n",
    "    min_freq: int = 2,\n",
    "    b_mode: str = \"B_raw\",        # \"B_raw\" or \"B_norm\"\n",
    "    also_b_norm: bool = True,     # export B_norm alongside chosen b_mode\n",
    "    export_a: bool = True,\n",
    "    max_id_vars: int = 64,\n",
    ") -> dict:\n",
    "    in_path = Path(input_path)\n",
    "    out_dir_p = Path(out_dir)\n",
    "    out_dir_p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) load + filter + dedup\n",
    "    raw_rows: List[RowView] = []\n",
    "    for d in read_jsonl(in_path):\n",
    "        rv = adapt_row(d)\n",
    "        if rv is None:\n",
    "            continue\n",
    "        if not default_filter(rv, language=language):\n",
    "            continue\n",
    "        raw_rows.append(rv)\n",
    "\n",
    "    # exact dedup by sha\n",
    "    seen = set()\n",
    "    dedup: List[RowView] = []\n",
    "    for rv in raw_rows:\n",
    "        if rv.sha in seen:\n",
    "            continue\n",
    "        seen.add(rv.sha)\n",
    "        dedup.append(rv)\n",
    "\n",
    "    # 2) split by repo (stable)\n",
    "    splits: Dict[str, List[RowView]] = {\"train\": [], \"val\": [], \"test\": []}\n",
    "    for rv in dedup:\n",
    "        s = split_by_repo(rv.repo, train=train_ratio, val=val_ratio)\n",
    "        splits[s].append(rv)\n",
    "\n",
    "    # 3) build vocab for B on train split only\n",
    "    token2id, id2token, freq = build_vocab(\n",
    "        splits[\"train\"], b_mode, vocab_size=vocab_size, min_freq=min_freq, max_id_vars=max_id_vars\n",
    "    )\n",
    "    write_json(out_dir_p/\"tokenizer_B\"/\"token2id.json\", token2id)\n",
    "    write_json(out_dir_p/\"tokenizer_B\"/\"id2token.json\", id2token)\n",
    "    write_json(out_dir_p/\"tokenizer_B\"/\"config.json\", {\n",
    "        \"mode\": b_mode,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"min_freq\": min_freq,\n",
    "        \"special_tokens\": SPECIAL_B,\n",
    "        \"task_tokens\": [\"<task:doc2code>\", \"<doc>\", \"</doc>\", \"<code>\", \"</code>\"],\n",
    "        \"max_id_vars\": max_id_vars\n",
    "    })\n",
    "\n",
    "    # 4) export splits + metrics\n",
    "    stats = {\n",
    "        \"input_path\": str(in_path),\n",
    "        \"kept_after_filter\": len(raw_rows),\n",
    "        \"kept_after_dedup\": len(dedup),\n",
    "        \"splits\": {k: len(v) for k, v in splits.items()},\n",
    "        \"max_len\": max_len,\n",
    "        \"variant_B\": {},\n",
    "        \"variant_A\": {},\n",
    "    }\n",
    "\n",
    "    def export_B(mode: str):\n",
    "        for split_name, rvs in splits.items():\n",
    "            encoded = []\n",
    "            oov_doc = 0\n",
    "            oov_code = 0\n",
    "            total_doc = 0\n",
    "            total_code = 0\n",
    "            lens = []\n",
    "            trunc = 0\n",
    "            parsable_ok = 0\n",
    "            parsable_total = 0\n",
    "\n",
    "            for rv in rvs:\n",
    "                inp_toks, tgt_toks = build_example_tokens(rv, mode, max_id_vars=max_id_vars)\n",
    "                inp_ids, _ = encode_tokens(inp_toks, token2id)\n",
    "                tgt_ids, _ = encode_tokens(tgt_toks, token2id)\n",
    "\n",
    "                # length policy: hard cut (truncate) to max_len, count as truncation\n",
    "                total_len = len(inp_ids) + len(tgt_ids)\n",
    "                if total_len > max_len:\n",
    "                    trunc += 1\n",
    "                    overflow = total_len - max_len\n",
    "                    if overflow > 0:\n",
    "                        if overflow >= len(tgt_ids):\n",
    "                            tgt_ids = [token2id[\"<eos>\"]]\n",
    "                        else:\n",
    "                            tgt_ids = tgt_ids[:-overflow]\n",
    "                            if tgt_ids and tgt_ids[-1] != token2id[\"<eos>\"]:\n",
    "                                tgt_ids[-1] = token2id[\"<eos>\"]\n",
    "\n",
    "                # OOV accounting: doc tokens and code tokens separately\n",
    "                doc_tokens = normalize_docstring(rv.doc).split()\n",
    "                _, doc_oov = encode_tokens(doc_tokens, token2id)\n",
    "                oov_doc += doc_oov\n",
    "                total_doc += len(doc_tokens)\n",
    "\n",
    "                code_tokens = python_struct_tokens(rv.code, mode=mode, max_ids=max_id_vars)\n",
    "                _, code_oov = encode_tokens(code_tokens, token2id)\n",
    "                oov_code += code_oov\n",
    "                total_code += len(code_tokens)\n",
    "\n",
    "                lens.append(len(inp_ids) + len(tgt_ids))\n",
    "\n",
    "                parsable_total += 1\n",
    "                det_code = detokenize_python(code_tokens)\n",
    "                if safe_ast_parse(det_code):\n",
    "                    parsable_ok += 1\n",
    "\n",
    "                encoded.append({\n",
    "                    \"repo\": rv.repo,\n",
    "                    \"path\": rv.path,\n",
    "                    \"func_name\": rv.func_name,\n",
    "                    \"sha256\": rv.sha,\n",
    "                    \"split\": split_name,\n",
    "                    \"mode\": mode,\n",
    "                    \"input_ids\": inp_ids,\n",
    "                    \"labels\": tgt_ids,\n",
    "                })\n",
    "\n",
    "            out_path = out_dir_p / f\"{split_name}.{mode}.jsonl\"\n",
    "            write_jsonl(out_path, encoded)\n",
    "\n",
    "            stats[\"variant_B\"].setdefault(mode, {})\n",
    "            stats[\"variant_B\"][mode][split_name] = {\n",
    "                \"n_examples\": len(rvs),\n",
    "                \"n_encoded\": len(encoded),\n",
    "                \"truncation_rate\": (trunc / max(1, len(rvs))),\n",
    "                \"len_quantiles\": quantiles_int(lens),\n",
    "                \"oov_rate_doc\": (oov_doc / max(1, total_doc)),\n",
    "                \"oov_rate_code\": (oov_code / max(1, total_code)),\n",
    "                \"parsable_after_detokenize_rate\": (parsable_ok / max(1, parsable_total)),\n",
    "                \"out_path\": str(out_path),\n",
    "            }\n",
    "\n",
    "    # export chosen B mode\n",
    "    export_B(b_mode)\n",
    "    if also_b_norm and b_mode != \"B_norm\":\n",
    "        export_B(\"B_norm\")\n",
    "\n",
    "    # export A if requested\n",
    "    if export_a:\n",
    "        for split_name, rvs in splits.items():\n",
    "            encoded = []\n",
    "            lens = []\n",
    "            trunc = 0\n",
    "            for rv in rvs:\n",
    "                text = build_line_text(rv)\n",
    "                ids, _, tr = encode_bytes(text, max_len)\n",
    "                if tr:\n",
    "                    trunc += 1\n",
    "                lens.append(len(ids))\n",
    "                encoded.append({\n",
    "                    \"repo\": rv.repo,\n",
    "                    \"path\": rv.path,\n",
    "                    \"func_name\": rv.func_name,\n",
    "                    \"sha256\": rv.sha,\n",
    "                    \"split\": split_name,\n",
    "                    \"mode\": \"A_bytes\",\n",
    "                    \"input_ids\": ids,\n",
    "                })\n",
    "            out_path = out_dir_p / f\"{split_name}.A_bytes.jsonl\"\n",
    "            write_jsonl(out_path, encoded)\n",
    "            stats[\"variant_A\"][split_name] = {\n",
    "                \"n_examples\": len(rvs),\n",
    "                \"truncation_rate\": trunc / max(1, len(rvs)),\n",
    "                \"len_quantiles\": quantiles_int(lens),\n",
    "                \"vocab_size\": BYTE_OFFSET + 256,\n",
    "                \"out_path\": str(out_path),\n",
    "            }\n",
    "        write_json(out_dir_p/\"tokenizer_A\"/\"config.json\", {\n",
    "            \"special_tokens\": SPECIAL_A,\n",
    "            \"byte_offset\": BYTE_OFFSET,\n",
    "            \"vocab_size\": BYTE_OFFSET + 256\n",
    "        })\n",
    "\n",
    "    write_json(out_dir_p/\"stats.json\", stats)\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea29253e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:3: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:5: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:19: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<unknown>:19: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:11: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:13: SyntaxWarning: invalid escape sequence '\\;'\n",
      "<unknown>:12: SyntaxWarning: invalid escape sequence '\\;'\n",
      "<unknown>:104: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<unknown>:113: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<unknown>:11: SyntaxWarning: invalid escape sequence '\\['\n",
      "<unknown>:37: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:13: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:8: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:21: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:22: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:24: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:28: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:31: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:34: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:38: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:40: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:8: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<unknown>:30: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:30: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<unknown>:4: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:19: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:15: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:18: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:25: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:16: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:16: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:18: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:46: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<unknown>:10: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:42: SyntaxWarning: invalid escape sequence '\\('\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\,'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:14: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:16: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\#'\n",
      "<unknown>:27: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:30: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\i'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<unknown>:4: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<unknown>:2: SyntaxWarning: invalid escape sequence '\\<'\n",
      "<unknown>:5: SyntaxWarning: invalid escape sequence '\\g'\n",
      "<unknown>:34: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<unknown>:7: SyntaxWarning: invalid escape sequence '\\w'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'train': 12032, 'val': 2010, 'test': 1177}, 'out_tokenization')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Run \n",
    "# -------------------------\n",
    "\n",
    "INPUT_JSONL = \"/Users/polzovatel/projects/GAN/GAN-and-Diffusion-Models-EDA/csn_python_jsonl_full/csn_test_clean.jsonl\"  \n",
    "OUT_DIR = \"out_tokenization\"\n",
    "\n",
    "# Вариант B: основной режим\n",
    "B_MODE = \"B_raw\"   # or \"B_norm\"\n",
    "\n",
    "stats = run_tokenization(\n",
    "    input_path=INPUT_JSONL,\n",
    "    out_dir=OUT_DIR,\n",
    "    language=\"python\",\n",
    "    max_len=2048,\n",
    "    train_ratio=0.90,\n",
    "    val_ratio=0.05,\n",
    "    vocab_size=32000,\n",
    "    min_freq=2,\n",
    "    b_mode=B_MODE,\n",
    "    also_b_norm=True,\n",
    "    export_a=True,\n",
    "    max_id_vars=64,\n",
    ")\n",
    "\n",
    "stats[\"splits\"], OUT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20af6ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
